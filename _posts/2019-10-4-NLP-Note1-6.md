---
layout:     post
title:      cs224n自然语言处理公开课笔记（六）
subtitle:   Lecture 13 -  ConvNet for NLP
date:       2019-10-3
author:     Yunlongs
catalog: true
tags:
    - cs224n
    - 自然语言处理
---

>视频地址：https://www.bilibili.com/video/av41393758/?p=13
课程主页地址：http://web.stanford.edu/class/cs224n/
课程讲义下载地址：https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/Books/CS224n/cs224n-2019-lecture11-convnets.pdf

# Lecture 13 -  NLP中的卷积神经网络应用

## 1. 从RNN到CNN

**NLP为什么需要CNN？（RNN在NLP中存在哪些不足？）**
- RNN通常不能捕捉那些没有前面上下文的句法结构
- RNN通常会在最后产生的向量中捕捉了太多了过去单词的信息。

**CNN的主要思想：**
- 如果我们只通过对每个固定长度单词序列进行计算，产生最后的向量
- 将句子，按照n-gram进行切分。例如：“tentative deal reached to keep government open”
可以切分为：tentative deal reached, deal reached to, reached to keep, to keep government, keep government open
- 不用管句子的语法结构
- 易于并行化

## 2. CNN在NLP中如何工作
### 2.1 1维卷积在文本中的例子

假设每个单词的词向量维度为4，则对于如下7个单词来说，可以形成$7 \times 4$的输入矩阵，而我们选用的卷积大小为3，为$3 \times 4$的矩阵，即每次对3个词做卷积操作。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/42.png)

### 2.2 带padding的1维卷积在文本中的例子

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/43.png)
在上面的那种情况下，最左边的词仅进行了一次卷积操作，而中间的词会进行多次的卷积操作，这样就导致有可能丢失边缘单词的信息，所以我们需要在单词的边界上添加padding。

添加padding 的方式可以使在单词的边界上添加一层或多层的0向量，来保留边缘词的信息。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/44.png)

### 2.3 多filter的1维卷积情况
一个filter可以对输入矩阵做一种情况的处理，如果我们想要同时对输入的文本做多种方式的处理，并且保留这多种方式处理后的结果，则我们可以同时应用多个filter，将得到的卷积结果并在一起，得到一个新的结果矩阵：

例如，下图就是对输入矩阵使用了3个不同的filter：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/45.png)

### 2.4 一维卷积后的池化层
和cv中的一样，我们可以在卷积后添加一层maxpooling，用来提取主要的特征。
$Input -> Conv1d -> maxpooling->result $
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/46.png)

注意这里的maxpooling可以设定选定最大值的个数，即k-max
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/48.png)

或者说添加一层avgpooling，用来去除噪声：
$Input -> Conv1d -> avgpooling->result $
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/47.png)

## 3. 单层CNN for 句子分类

源于[Yoon Kim (2014): Convolutional Neural Networks for Sentence 
Classification. EMNLP 2014.](https://arxiv.org/pdf/1408.5882.pdf)

目标：
- 判断句子的正面或负面情感
- 问题分类

**符号定义：**
- 词向量$\mathbf{x}_ {i} \in \mathbb{R}^{k}$
- 句子$\mathbf{x}_ {1: n}=\mathbf{x}_ {1} \oplus x_ {2} \oplus \cdots \oplus \mathbf{x}_ {n}$
- 卷积filter：$\mathbf{w} \in \mathbb{R}^{h k}$（filter大小为h）

**多通道卷积运行流程：**
- 1. 通过训练好的词向量进行初始化
- 2. 将每个词的词向量做一个拷贝，作为第二个通道
- 3. 使用设定好的卷积filter对每个通道进行操作
- 4. 最后面添加一层softmax
- 5. 仅对其中一个通道进行反向传播

**CNN用于文本分类的模型如下：**

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/49.png)

## 4. 模型比较
- **Bag of Vectors：** 对分类问题来讲非常棒的baseline模型，接几层ReLU甚至可能打败CNN。
- **Window Model：** 对单个单词的分类来讲挺好，但不适用于需要更多上下文的任务。
- **CNN：** 适合分类，不清楚如何做短语级别的标注，对短文本需要padding，难以用NLP的视角解释，容易GPU并行化
- **RNN：** 从左读到右，最符合认知。不是分类任务的最佳选择，比CNN慢，但可以做序列标注。

## 5. CNN的应用：翻译
使用CNN做encoder，RNN做Decoder
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/50.png)

## 6. 深度卷积网络用于文本分类
出发点：
- NLP中所有的模型层数都不是特别深，不像计算机视觉中存在很深层的网络
- 如果像计算机视觉一样，为NLP建立一个深度网络系统会怎么样？
- 在字符层面工作

VD-CNN的网络结构如图：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/51.png)

对于每个convolutional block有：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/52.png)