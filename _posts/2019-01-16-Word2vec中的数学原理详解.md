---
layout:     post
title:      Word2vec中的数学原理详解笔记
subtitle:   The mathematics in word2vec.
date:       2019-01-16
author:     Yunlongs
header-img: img/post-bg-w2vec.jpg
catalog: true
tags:
    - Word2vec
    - 机器学习
---





 
>这篇文章我将按照本书中从上到下中重要的知识点顺序编写，word2vec，简而言之，就是讲自然语言中的词(word)转换成数学计算可用的向量(vector)，本书请点击链接下载[《word2vec中的数学原理》](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/Books/word2vec-%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3.pdf),为了方便理解以及找到重点，我在此书中使用红线做了适当的圈画及注释，使用wps2019应该可以显示。

# Word2vec中的数学原理详解笔记
## 预备知识

**基本的数学公式和数据结构知识**
### 2.1 sigmoid函数
当成一个数学函数来理解，是神经网络中常用的**激活函数**之一。


![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/post-bg-w2vec-sigmoid.jpg)




![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-sigmoid-graph.jpg)




从图中可以看出，**x大于0，y大于0.5；x小于0，y小于0.5**，可应用于后续的二分类问题。

**数学性质：**
- 类奇函数，值域（0-1），定义域无穷

- ![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-sigmoid-dao.jpg)
- ![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-log-sigmoid.jpg)
- ![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-log-1-sigmoid.jpg)


### 2.2逻辑回归
在这里我将其简单的理解为**二分类问题**:在样本集｛x,y｝中，yi=1相对应的xi为**正例**，yi=0相对应的样本xi为**负例**.
二分类的**假设函数**如下：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-logical-return.jpg)
其中θ为待定参数，令x0=1可简写为
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-simple-h.jpg)
当阈值取T=0.5时,二分类的判别公式为
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-judge-logicalRe.jpg)
这里可以理解为，当计算出的h(x)>T时，y就是正例，否则，y是负例。

分类过程中的**参数θ的求法**：对如下**整体损失函数**进行优化，得到最优的参数θ*进行计算。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-all-cost.jpg)

优化的方法，感觉是利用极大似然估计法，来求得未知参数的极大值或极小值。
**单个样本**的损失函数cost(xi,yi)的**对数似然函数**为：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-log-cost.jpg)

故，若将上述5个公式编号，整个二分类问题的执行过程如下：5->1->4->2->3。

### 2.3 贝叶斯公式
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-bayes.jpg)

### 2.4 哈夫曼编码
每次选最小的两个节点合并
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-huffman-1.jpg)

进行编码（左1，右0）
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-Huffman-2.jpg)

## 3.背景知识
**自然语言处理（Nature Language Processing,NLP）**
### 3.1 统计语言模型
**统计语言模型**是用来*计算一个句子概率*的**概率模型**。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-statical-Language-mode.jpg)
利用**贝叶斯公式**，可链式分解为：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-statical-Language-2.jpg)

从上式中可以看出，统计语言模型的**实质**就是再求每个单词的**条件概率**，这些条件概率就是模型的**参数**，有了这些参数，就可以求得一个句子出现的概率。

### 3.2 n-gram模型
n-gram模型的**实质**就是将概率转化为次数，且每个单词仅与前n个单词相关。如下两图所示：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-n-gram-1.jpg)
统计语言模型的参数。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-n-gram-2.jpg)

---
考虑如下问题：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-n-gram-3.jpg)
因此，需要进行**平滑化处理**。

### 3.3 机器学习通用招数
在机器学习领域有一种*通用的招数*是这样的：对所考虑的问题建模后先为其构造一个**目标函数**，然后对这个目标函数**进行**优化，从而求得一组**最优的参数**没，最后利用这组最优参数对应的模型来**进行预测**。
`问题建模-->构造目标函数-->优化-->最优参数-->预测`

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-goal-function.jpg)

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-log-goal-function.jpg)

有了**似然函数（即目标函数）**，即可利用**极大似然估计法**，来对函数进行**最大化**。


![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-F-function.jpg)

其中θ是待定的参数集，优化目标函数即可获得到**最优参数集θ***。
得到了最优参数，即可利用对应的统计语言模型进行预测。

### 3.4神经概率语言模型

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-neural-network.jpg)


### 3.5 词向量的理解

**词向量的表示方式:**
1. one-hot representation
**词向量的维数**即词典D中所有的词的**数量**，一个词在词典D中是索引中的第几个，词向量中第几个分量为1，其他分量为0

2. Distributed Representation
基本思想：通过训练将**某种语言中的每一个词**映射成一个**固定长度**的较短向量，所有这些向量构成一个**词向量空间**，每一个向量则可视为该空间中的一个点。即可通过点之间的**“距离”**来判断词之间的相似性。

**如何获取词向量?**
LSA(Latent Semantic Analysis)和LDA(Latent Dirichlet Allocatiojn).以及下图。这里的原理并未提及。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-get-vector.jpg)

## $4 基于Hierarchical Softmax的模型
### 4.1 CBOW模型
**即求p(w|context(w))**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-CBOW-1.jpg)
#### 4.1.1 网络结构
1. **输入层：**context(w)中w前后的2c个词向量

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-output-CBOW.jpg)

3. **输出层**： 一颗二叉树，词是叶子节点。

### 4.1.2 梯度计算
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-grad-1.jpg)
从根节点到目的节点之间的每一次分支都可以看成进行了一次**二分类**，分到左边的是**负类**，右边的是**正类**。
根据逻辑回归，分到正类的**概率**是:
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-true-class.jpg)
分到负类的**概率**是：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-false-class.jpg)

**对一词典D中的任意词w，在Huffman树中必有根节点到w节点的唯一路径pw，路径pw上存在lw-1次分支，每次分支都是一次二分类，将路径上所以分类的概率连乘起来，就是所需的p(w|contexe(w))**

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-grad-2.jpg)

#### 4.1.3 随机梯度上升法
将由**梯度计算**得到的梯度（p(w|Context(w)))函数做其对数似然函数
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-radom-grad.jpg)
可*简写*为
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-radom-grad-simple.jpg)
这就是CBOW模型的**目标函数**，使这个**函数最大化（优化）**,这里采用的是**随机梯度上升法**.
首先**梯度的计算**：
对上式中两个未知参数**Xw，θ求偏导**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-dao-grad-%CE%B8.jpg)

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-dao-grad-x.jpg)

----
两个参数的**更新公式**:
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-renew-%CE%B8.jpg)
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-renew-x.jpg)

### 4.2 Skip-gram模型
**即求p(Context(w)|w)**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-skip-gram-1.jpg)

#### 4.2.1 网络结构
>与CBOW类似，不过投影层多余

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-skip%3Dgram-2.jpg)

#### 4.2.2 梯度计算
**目标函数**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-Skip-gram-3.jpg)

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-skip-gram-4.jpg)

对目标函数做其**对数似然函数**，并简化为：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-skip-gram-5.jpg)

梯度计算（略）
更新公式：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-skip-gram-6.jpg)
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-skip-gram-7.jpg)

## $5 基于Negative Sampling的模型
NEG不再采用复杂的Huffman树，而是利用相对简单的**随机负采样**,大幅提高性能。
### 5.1 CBOW模型
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-CBOW-1.jpg)

`对于给定的一个正样本(Context(w),w),目标函数为`
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-CBOW-2.jpg)
其中，
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-CBOW-3.jpg)
对g（w）最大化时，可以**增大正样本的概率同时降低负样本的概率**，故可以作为**整体优化的目标**。
取g(w)的**对数似然函数**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-CBOW-4.jpg)

----
**计算梯度，并可得更新公式**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-CBOW-5.jpg)
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-CBOW-6.jpg)


### 5.2 Skip-gram模型
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-skipgram-1.jpg)
最大化g(w),做**g(w)的对数似然函数**：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-skipgram-2.jpg)

-----
**得到更新公式**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-skipgram-3.jpg)

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-skipgram-4.jpg)

### 5.3 负采样算法
**负采样即给定一个词w，如何生成NEG(W)?**
>负采样的过程本质是一个带权采样的过程

将[0-1]根据每个次的长度（出现次数）不等均分为In。
再根据词的个数n等分为Mn。建立的映射关系如下：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/w2vec-NEG-sample.jpg)

生成随机数，根据映射m寻找相应的I进行采样。

**由此，对于高频词，被选为负样本的概率就大些，对于低频词，被选为负样本的概率就小些。**
