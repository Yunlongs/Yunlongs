---
layout:     post
title:      cs224n自然语言处理公开课笔记（四）
subtitle:   Lecture 9 -  GRU and LSTM
date:       2019-10-2
author:     Yunlongs
catalog: true
tags:
    - cs224n
    - 自然语言处理
---

>视频地址：https://www.bilibili.com/video/av41393758/?p=9
课程主页地址：http://web.stanford.edu/class/cs224n/
课程讲义下载地址：https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/Books/CS224n/cs224n-2019-notes05-LM_RNN.pdf

# Lecture 9 -  GRU and LSTM

## 1. Gated Recurrent Units
因为循环神经网络具有无法捕捉深层的连接关系，且就有梯度消失的可能性，这里的GRU单元改变了传统的RNN隐藏层结构，使其可以更好地捕捉深层连接，并改善了梯度消失问题。

### 1.1 GRU单元的前向传播
**一个GRU单元的表达式为：**
$$\Gamma_{u}=\sigma\left(W_{u}\left[c^{<t-1>}, x^{<t>}\right]+b_{u}\right) \tag{update门}$$
$$
\Gamma_{r}=\sigma\left(W_{r}\left[c^{<t-1>}, x^{<t>}\right]+b_{r}\right) \tag{reset 门}
$$
$$
\tilde{c}^{<t>}=\tanh \left(W_{c}\left[\Gamma_{r} * c^{<t-1>}, x^{<t>}\right]+b_{c}\right) \tag{new memory门}
$$
$$
c^{<t>}=\Gamma_{u} * \tilde{c}^{<t>}+\left(1-\Gamma_{u}\right) * c^{<t-1>} \tag{activate 门}
$$
其中$a^{<t>}=c^{<t>}$。
其中每个门的具体作用为：
- **Update 门**： 更新信号$\Gamma_u$决定了上一隐藏层的状态$c^{<t-1>}$有多少应当传递到下一状态$c^{<t>}$中，即是否进行更新。例如，如果$z_{t} \approx 1$，那么$c^{<t-1>}$将近完全带入到下一状态中；如果$z_{t} \approx 0$，那么新的记忆单元$\hat c^{<t>}$将会传递到下一状态里。
- **Reset 门：** 重置信号$\Gamma_r$决定了上一状态$c^{<t-1>}$有对于新的记忆单元$\hat c^{<t>}$的重要性。如果reset门发现过去的隐藏层状态和当前新的记忆单元没有关系，其完全可以消除过去的隐藏层状态$c^{t-1}$。
- **New memory 门：** 是上一隐藏层传递的状态和新的输入词$x^{<t>}$之间的结合。更形象的说，通过联合上一隐藏层状态$c^{<t-1>}$和当前输入词$x^{<t>}$的影响，用来来生成新的隐藏层的状态。


**GRU单元更形象的结构图为：**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/29.png)

### 1.2 GRU单元的反向传播

## 2. LSTM（Long-Short-Term-Memories）
长短期记忆单元是另一类复杂的激活单元，其在结构上和GRU单元有些小小的不同，其具有更加强大的记忆能力和抗梯度消失能力。 
### 2.1 LSTM单元的前向传播
**一个LSTM单元的前向传播表达式为：**
$$
\begin{array}{ll}\tilde{c}^{<t>}=\tanh \left(W_{c}\left[a^{<t-1>}, x^{<t>}\right]+b_{c}\right) &\text {(new memory门)}
\\ \Gamma_{i}=\sigma\left(W_{u}\left[a^{<t-1>}, x^{<t>}\right]+b_{u}\right) &\text {(input 门)}
\\ \Gamma_{f}=\sigma\left(W_{f}\left[a^{<t-1>}, x^{<t>}\right]+b_{f}\right) &\text{(forget 门)}
\\ c^{<t>}=\Gamma_{i} * \tilde{c}^{<t>}+\Gamma_{f} * c^{<t-1>} &\text{(final memory 门)}
\\ \Gamma_{o}=\sigma\left(W_{o}\left[a^{<t-1>}, x^{<t>}\right]+b_{o}\right) &\text{(output 门)}
\\a^{<t>}=\Gamma_{o} * c^{<t>} 
\end{array}
$$

其中每个门的具体作用为：
- **New memory generation：** 与GRU类似，使用当前词语$x^{<t>}$ 和之前的隐状态$a^{<t-1>}$来生成新的记忆$\hat c^{<t>}$。于是新记忆里面就包含了当前词语$x^{<t>}$的属性。
- **Input Gate:**: 使用当前词语和之前的隐状态**决定当前词语是否值得保留用来产生*new memroy 门*的新记忆**，这个“是否”是通过$\Gamma_i$来体现的
- **Forget Gate**：和Input Gate类似，只不过它不是用来衡量输入单词的有用与否，**而是衡量过去的记忆对计算当前记忆有用与否。** 它接受输入单词和上一刻的隐状态产生输出 $\Gamma _f$。
- **Final memory generation**：根据Input Gate的建议决定保留多少新产生的记忆$\hat c^{<t>}$，根据forget Gate的建议决定忘掉多少过去得记忆$c^{<t-1>}$
- **Output Gate:** 在GRU中不显示存在，其作用是将final memory$c^{<t>}$和隐藏层的状态$a^{<t-1>}$分离开来。因为final memory可能包含了很多不是那么重要的信息，没有必要全传递到下一隐藏层状态$a^{<t>}$中去，因此它根据$a^{<t>}$需要那一部分的$c^{<t>}$，从而来进行分配。

**LSTM单元的网络结构**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/30.png)