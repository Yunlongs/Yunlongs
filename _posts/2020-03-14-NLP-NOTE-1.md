---
layout:     post
title:      NLP系列近五年突破技术（一）笔记
subtitle:   Lecture 1 - 词向量与ELMo模型
date:       2020-03-14
author:     Yunlongs
catalog: true
tags:
    - 机器学习
    - 深度学习
    - “ 共同战疫” NLP系列专题 直播 
---

>课程信息：感谢@李文哲老师，贪心科技--“ 共同战疫” NLP系列专题 直播课，本文所有版权归贪心科技https://www.greedyai.com/所有。

# Lecture 1 - 词向量与ELMo模型
## 一.词向量分布
### 1. 语言模型
计算句子的概率或者一系列单词的概率：

$P(全民AI是趋势)=P(全民,AI,是,趋势)=P(全民) \times P(AI | 全民) \times P(是|全民,AI) \times P(趋势|全民,AI,是)$

语言模型需要最大化上面的似然概率，即基于训练好的模型，可以基于“全民”这个单词可以猜出“AI”这个单词，基于“全民AI”可以预测出“是”....从而预测出整个句子。

### 2. 基于分布式的词向量表示模型总览
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-1/1.png)

**Global vs Local 方法**： 
Global方法：将整个数据集输入模型之中，从而学习到词的全局属性
- Advantange:全局
- Disadvantage:1,计算大 2,不能在线学习

Local方法: 比如Skip-gram，基于窗口的局部方法
- Advantage: 随时增减，方便应用在大数据
- 不能全局think问题

## 二.词向量训练的常见方法
### 1.基于非语言模型方法
**Skip-gram:** 通过中心词预测上下文。详情见我的文章：
**CBOW**:通过上下文预测中心词。详情见我的文章：

**NNLM方法：** 给定句子 $s= w_1, w_2,w_3,w_4,w_5$。
通过最大化$P(w_2|w_1)\times P(w_3|w_2)\times P(w_4|w_3)\times P(w_5|w_4)$（二阶马尔科夫假设）来进行学习。

### 2.词向量的多义性问题
通过前面的方法训练出的每一个单词的词向量是**固定的**，但是在一些句子中一个单词通常代表不知一个意思，例如下句：
>Yesterday,when he **backed** the car,he hurt his **back**.

**所以，如何学出一个单词在不同上下文中的词向量呢？**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-1/2.png)
答案：ABCD