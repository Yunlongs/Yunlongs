---
layout:     post
title:      cs224n自然语言处理公开课笔记（五）
subtitle:   Lecture 10 -  NMT and Attention Model
date:       2019-10-3
author:     Yunlongs
catalog: true
tags:
    - cs224n
    - 自然语言处理
---

>视频地址：https://www.bilibili.com/video/av41393758/?p=10
课程主页地址：http://web.stanford.edu/class/cs224n/
课程讲义下载地址：https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/Books/CS224n/cs224n-2019-notes06-NMT_seq2seq_attention.pdf

# 1.Neural  Machine Translation with Seq2Seq
>神经网络在机器翻译方面的应用，这一部分还是直接看吴恩达的笔记即可 [序列模型和注意力机制(Sequence models & Attention mechanism）](https://baozoulin.gitbook.io/neural-networks-and-deep-learning/di-wu-men-ke-xu-lie-mo-xing-sequence-models/di-wu-men-kexulie-mo-578b28-sequence-models/di-san-zhou-xu-lie-mo-xing-he-zhu-yi-li-ji-zhi-ff08-sequence-models-and-attention-mechanism)
以上链接总共涉及了以下知识点：
- 条件语言模型
- greedy search
- Beam search
- 改进Beam search
- Beam search的误差分析
- Bleu得分
- 注意力模型

剩下将补充cs224n里其他的知识点，和注意力模型。

# 2.Attention Model
## 2.1 注意力模型的动机

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/34.png)
**动机：**  当句子越来越长时，神经网络将整个句子记忆下来的能力会越来越差。而**注意力模型**翻译得很像人类，一次翻译句子的一部分。且机器翻译系统只会翻译句子的一部分，不会有上图中一个巨大的下倾（huge dip）。

## 2.2 网络结构
