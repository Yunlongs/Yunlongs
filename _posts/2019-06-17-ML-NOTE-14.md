---
layout:     post
title:      吴恩达Stanford机器学习公开课（十四）笔记
subtitle:   Lecture 14 - Principal components analysis
date:       2019-06-17
author:     Yunlongs
catalog: true
tags:
    - 机器学习
    - 吴恩达Stanford机器学习公开课
---

>视频地址：https://www.bilibili.com/video/av49432977/?p=14
课程主页地址：http://cs229.stanford.edu/
课程讲义下载地址：https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/Books/cs229-notes10.pdf

# Lecture 14 - 主成成分分析(Principal components analysis)

>在本部分内容我们将介绍一种PCA方法，同样可以识别出数据所约在的子空间，但是其更为直接，只需要一个特征向量的计算，且不需要求助于EM。

给定以下**数据集**： $\left\{x^{(i)} ; i=1, \ldots, m\right\}$为m个不同类型汽车的属性，比如说最大速度、转向角等。 $x^{(i)} \in \mathbb{R}^{n}$$(n \ll m)$。但是，假如说有两个属性--$x_{i}$和$x_j$--分别为米每时和千米每时，这样这两个属性就具有了一定的线性依赖，所知这些数据其实是分布在n-1维的子空间中。所以我们该**如何去除这些冗余呢？**

给定**另一组更具体的例子**，设$x_{1}^{(i)}$为飞行员i的驾驶能力，$x_{2}^{(i)}$是他对飞行的喜爱程度，已知这两个属性具有很强的相关性（只有真正喜欢飞行的才能拥有更高超的飞行能力）。现在将一些样本分别以x1和x2位坐标轴绘制在图上，沿着u1方向能更好的捕捉一个人的驾驶能力，u2方向则为一些噪声，那么我们**如何来自动捕捉出u1方向呢？**
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-1.jpg)

**在PCA算法中，需要先对数据进行预处理步骤，即规范化其均值和方差：**
1[](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-2.jpg)
其中，**(1-2)步的作用为** 使数据的均值变为0，**(3-4)步的作用为** 单位化方差，使不同类型的属性能够在同一个规模上度量。当我们已知所有数据的属性都在相同的规模上时，步骤(3-4)可以省略，例如灰度图中的每一个点$x_{j}^{(i)}$均在$\{0,1, \ldots, 255\}$范围内取值，来表示对于图像i中第j个像素点的密度值。

**规范化之后，如何计算出数据所在的主轴u呢？** 一种方法是寻找一个单位向量u，使得样本点在其上投影所得的方差最大化，这样的投影是十分具有信息量的。

已经规范化的数据集样本点如下：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-3.jpg)

现在令图中的实线为我们所选取的方向u，实心点为样本点在u上的投影。如下图所示，
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-4.jpg)
`我们可以从图中看出来，投影点距离原点的距离很远，且在方向u上的分布具有很大的方差`

![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-5.jpg)
`这幅图里，投影在方向u上的方差更小了，并且更接近于原点`

因此，我们**愿意去自动化选取第一幅图中哪个具有较大方差的方向u**。当给定单位向量u和点x时，x在u上的投影长度为$x^{T} u$(也就是投影点到原点的长度)，因此，为了最大化投影的方差，我们需要选择一个单位长度的向量u来最大化：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-6.jpg)

我们可以轻松的辨别出最大化上面的等式可以得到，u就是协方差矩阵$\Sigma=\frac{1}{m} \sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}$的主特征向量。
>知识点补充：**主特征向量**为最大的那个特征值所对应的特征向量。
**推导如下**：原式子即为条件最值问题
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-7.jpg)
建立的拉格朗日乘数公式为：
$L(u,λ)=u^T\Sigma u-λ(u^Tu-1)$
得到，$\nabla_{u}L = \Sigma u-λu = 0$

总结一下，如果我们想要去寻找数据所在的1维子空间，我们选择u为$\Sigma $的主特征向量；扩展开来，如果我们想要投影我们的数据到k维子空间中$(k<n)$，我们需要选择$u_{1}, \dots, u_{k}$来成为$\Sigma$的k个特征向量。所以这些$u_{i}$就成了新的数据的正交基。

然后，为了展示在这些基础上的$x^{(i)}$，我们仅需要计算如下向量：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/Stanford/lecture-14-8.jpg)

从上可以看出来，虽然$x^{(i)} \in \mathbb{R}^{n}$，但是$y^{(i)}$给了更小维度的$x^{(i)}$的估计，因此，**PCA也被称为维度降低算法**。向量$u_{1}, \dots, u_{k}$同时也被称作为数据的**第k个主成成分**。

**Remark**： 在上面的建模过程中，我们使用的是k=1情况下的例子，使用特征向量的属性我们可以很简单的推广到所有可能的正交基$u_{1}, \dots, u_{k}$，这是只需要最大化$\sum_{i}\left\|y^{(i)}\right\|_{2}^{2}$。因此在我们对基的选择过程中尽可能的保留了原始数据的可变性。

>在problem set4，中会有PCA的其他导出方法

## PCA的应用
**1. 压缩
2. 可视化
3. 降维
4. 面部识别**

**压缩：** 使用更低维度的$y^{(i)}$来表示$x^{(i)}$。
**可视化：** 当把高维度的数据降低到k=2或者k=2维时，我们可以将$y^{(i)}$绘制出来，来观察每一聚类的相似程度。
**降维：** 预处理学习算法索要使用的数据集，这样可以降低计算难度，还可以降低假设类的复杂程度和避免过拟合。
**面部识别：** 在一副图片中，$x^{(i)} \in \mathbb{R}^{100 \times 100}$，为10000维的向量，使用PCA我们可以替换为更低纬度的$y^{(i)}$。在这样做的过程中，我们希望我们发现的主要组件保留了一个人真正看起来像的脸之间的有趣的、有系统的变化，而不是图像中的“噪声”引入轻微的照明变化，略有不同的成像条件，等等。然后我们在低纬度空间中，通过计算$\left\|y^{(i)}-y^{(j)}\right\|_{2}$来计算不同面部i和j之间的变化。这可以得到一个效果好的面部识别算法。
