---
layout:     post
title:      NLP系列近五年突破技术（二）笔记
subtitle:   Lecture 2 - Self-Attention与Transformer
date:       2020-03-15
author:     Yunlongs
catalog: true
tags:
    - 机器学习
    - 深度学习
    - “ 共同战疫” NLP系列专题 直播 
---

>课程信息：感谢@袁源老师，贪心科技--“ 共同战疫” NLP系列专题 直播课，本文所有版权归贪心科技https://www.greedyai.com/所有。

# Lecture 2 - Self-Attention与Transformer

## 1. 背景介绍
（1）之前模型的缺点：本质原因还是RNN体系的梯度消失／爆炸问题，尽管LSTM在一定程度上能缓解该问题，然而并未能完全避免。
（2）计算量的问题：因为时序模型必须是串行的，可能在预测阶段还能接受这个时效性，但是在训练阶段，与能并行计算的CNN相比，时效性表现的就不尽如人意了。

## 2.Transormer网络结构
Transformer内部由6个Encoder和6个Decoder组成。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/1.png)

### Encoder和Decoder的内部结构
每个Encoder分为Self-Attention层和前馈网络层，Decoder接受Encoder的输入并且中间多了一层Attention。

如下图，**深度仅为一层** 的Transformer的Encoder结构图
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/2.png)

### Encoder
对于一个Encoder的内部结构如图，输入词$x_1$和词$x_2$的embedding，通过Self-Attention共同生成$z_1$和$z_2$，随后单独通过前馈网络得到$r_1$和$r_2
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/3.png)

## 3.Self-Attention
如果有下图所示的句子，将其分解为一系列的单词，每个单词都计算与其他单词之间的attention有多大。例如单词*it*与代词*The*和名次*animal*之间的注意力权重最大。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/4.png)

### Attention计算
**我们现在来理解Attention是如何计算的**
**第一步：** 首先根据公共权值矩阵$W^Q,W^K,W^V$来计算出每个单词embedding所对应的*Queries、Keys、Values*是那个向量。
$q_1 = x_1 \times W^Q$,
$k_1 = x_1 \timse W^K$,
$v_1 = x_1 \times W^V$。
所有的词都共用同一个权重矩阵
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/5.png)

**第二步：** 根据如下步骤即可计算出每个输入embedding的attention向量$z$。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/6.png)

**对Queries、Keys和Values的解释：** 最初用于信息检索领域：
此时有查询q="5G"
和两个键-值对：K=>V : "5G" =>“Huawei”
K=>V : "4G"=>"Nokia"

可以看出来检索出“Huawei”的概率更大一些，所以为了能使两个不相关的词$x_1,x_2$能够相互检索，这里就可以使用公共权重矩阵来建立联系。公共权重矩阵最开始是随机化的，在之后的学习过程中逐渐学习到适当的模式。

### 向量化
为了加快运算，当然要进行向量化。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/7.png)

上述Attention计算的过程就为
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/8.png)

## 4. “multi-headed” attention
之前我们说，所有的词嵌入在计算attention时同用三个公共权重矩阵$W^Q,W^K,W^V$，那么如果我们像下图一样使用多个公共权重举证多计算几次，会有什么效果？
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/9.png)

比如，现在我们采用8个不同的公共权重矩阵，产生了8个不同的Attention输出：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/10.png)

再将这8个不同的Attention heads拼接起来，乘以权重矩阵$W^o$，最后可以得到捕捉了所有attention heads的$Z$矩阵，和单head attention达成了格式上的统一。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/11.png)

因此，总结一下，self-attention的整个计算过程如下示意图，
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/12.png)

**但是multi-headed 能达到什么样的效果呢？**
如下图，我们提取了8个head中的两个head进行展示：对于单词“it”来说，橘色部分的“The”和“animal”表达此head关注与两单词之间的指代关系，而绿色部分的“was”和“tired”表示此head更关注两单词之间的状态关系。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/13.png)

## 5.Positional encoding
需要注意的是，在我们整个transformer的框架中的输入为词嵌入与其*positional encoding*所得，那么这个*positional encoding*是干什么的？
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/14.png)

其编码规则如下：
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/15.png)

例子如下：级sin与cos交替，分母逐渐除以100...
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/16.png)

**为什么这样做？**
可以看出第一行仅为01交替，越往下更越像之前的线性组合，这样可以不同单词位置的编码不一样。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/17.png)

但是需要注意的是，*positional encoding*不一定使用作者的方法，选用其他的不同的位置编码也可以。

## 6.Residual，Layer Normalization
每一个Encoder内部，每一层都会有一个残参连接，并且带有一个层-归一化操作。
![](https://yunlongs-1253041399.cos.ap-chengdu.myqcloud.com/image/NLP/GreedyAI-2/18.png)